Ensemble  learning
refers to the process employed to train multiple learning machines and combine their outputs, treating them as committee of decision makers

The principles that the decision of the committee with individual predictions combined appropriately should have better overall accuracy  , on average than any individual committee mentor

The members of ensemble might be predicting
real valued numbers
class labels
posterior probabilities
rankings
clustering  etc.

an ensemble consists of a set of models and a method to combine them

the underlying principle of ensemble learning is a recognition that in real world situations, every machine learning model has limitations and will make errors

The aim of ensemble learning is to manage their strengths and weakness, leading to the  best possible decision being taken overall






model selection(which type of classifier lobe used )
1. always we cross valuation
2. for ensemble learning, one should choose direuse models
3. the outcomes of each model are usually averaged
4.model selection should be done while keeping in mind the final goat reduce risk
5.study the stability of your data sample data quantity

if we have plenty of data , we may split the data into two parts (train validity  test)
train each classifier on a separate subsets

if the dataset is not big in size use different bootstrap samples of data to train different classifiers

(here each bootstrap sample is the random sample of the data drawn with replacements and treated as if it was independently drawn from the underlying distributions )



divide and conquer

the decision boundary that separates the data may be too complex for certain problems

here, using  ensemble learning, the classification
system follows a divide and conquer approach.

divide the data space into smaller and easier to learn partitions  where each classifier learns only one of the simpler partitions

data fusions
suitable combinations of data from different source is called data fusion
data/information fusion can lead to improved accuracy (with the help of sources that may provide complementary information )

confidence estimations
if the majority of classifiers agree with their decision ensemble having high confidence in its decision
half make one decision and half make different decision ensemble having low confidence



it may be noted that ensemble having high confidence does not mean that decision is correct

however, properly trained ensemble decision is usually correct if confidence is high

ensemble learning algorithms
1. bagging/bootstrap aggregating
oldest and simplest
diversity is obtained by using bootstrap replicas

2. boosting
similar to bagging , boosting creates replicas by re sampling the data
however, in boosting  , re sampling is strategically geared to provide the most informative
training data for each consecutive classifier

each iteration- 3 weak classifiers
1. classifiers c1 trained with random subset
2. c2: trained on the most informative subset ( half of training data correctly classified by c1, and the other half miss classified )
3. c3: trained with instances on which c1 and c2 disagree



boosting is designed for binary class problems

adaptive boost (adaptive boosting)
instances are drawn into the subset database from an iteratively updated sample distributions
of the training data

the classifiers are combined through weighted majority voting where voting weights are based on classifiers
training errors


random forests
another version of bagging
ensemble of decision trees trained with a bagging mechanism
(each tree depends on a collection of random variables)

they naturally handle both regression and multicast classification

are relatively fast to train /predict
depends only on one or two tuning parameters






DECORATE (diverse ensemble creation of Opposition relabeling of artificial training examples )

based on specially constructed artificial examples

used to create diverse ensembles

Combining ensemble members
1. majority voting
2. weighted majority voting
3. broad count
